rm(list = ls())                               #Clean the Global Environment
cat ("\014")                                  #Clean the R console

# In this file:
# ==============================================================================
# 1. Run the analyses of the 2018 article of Kenny about dsAPIM
#    with a focus on the empty model. 
# 2. In the article there are equations for the DF calculation and F test, 
#    to extract the dyadic reciprocity from the model as performed in the SPSS. 
#    I add the generalized equations for uneven groups and missing values.
# 3. Perform some tests of all sorts:    
#    * run tripleR to generate SRM analysis
#    * I tried (unsuccessfully...) to add a limited dummy-codes dataset that 
#      mimics the DFs produced by the SPSS in the article. 
#      In the article, Kenny writes that to mimic it one should run the dummy-codes
#      effects' test once with partnerN with less dummy codes, and once with actorN.
#      But, the overall DFs generated by the R code, is different than the one
#      generated by the SPSS. The whole point of using the dummy codes was to 
#      correct the DFs, so why doesn't it bother anybody? 
#      To be honest, it's close. but, in my opinion, not close enough.
#    * I tried to generate an alternative to dummy codes. no good... (I wanted
#      to get information about the overall affect of the sets of dummy codes.)
#    * run the same analysis but with dyadic scores after extracting the SRM scores 
#      I didn't do it eventually, but I keep it in case I will complete it if 
#      necessary. The same analysis on scores is performed in the next code file,
#      on the Humility-performance decomposed data with all the suggested analyses.

source ("99_utility Functions.R")
ipak(c("tidyverse", "nlme"))

pw <- orig <- read.csv("dsAPIM data from Kenny 2018 paper.csv")
pw <- arrange(pw, Group)

# The unique identifiers are ActorN, PartnerN = paste Group, Actor/Partner
# The dyad is unique inside each group, but not over the dataset.

# Add dummy codes 
# ==============================================================================
  pw$ActorN <- as.factor(pw$ActorN)
  pw$PartnerN <- as.factor(pw$PartnerN)
  pw$Group <- as.factor(pw$Group)
  
  # Create dummy codes using model.matrix.
  Adum <- model.matrix(~pw$ActorN)
  Pdum <- model.matrix(~pw$PartnerN)
  
  #To avoid singularity:
    # we drop one dummy for Actor (the minimum to avoid singularity)
    Adum <- Adum[, -1] 
    # and drop a target column for each group.
    Pdum <- Pdum[, duplicated(pw[!duplicated(pw$PartnerN),"Group"])]
  
  # Add the dummy codes into the dataset:
  pw$Adum <- Adum
  pw$Pdum <- Pdum

# Run SRM on the data
# =============================================
  x <- select(orig, a.id = ActorN, p.id = PartnerN, Group, ends_with("_AB"))
  x <- as.data.frame(x)
  ipak("TripleR")
  (RR(Sociable_AB + Attractive_AB ~ a.id * p.id| Group, data = x))
  # the dyadic reciprocity (standardized) of sociable is 0.104. 
  # we will soon see it in the empty model's rho.
  
  
# The empty model
# ==============================================================================
  # * look at the empty model and what we can learn from it.
  # * Explain the F values of the dummies 
  # * Play with the empty model (the form, empty model for attractive, the coefficients)
  
  # correlation=corCompSymm: 
  #       Calculate the correlation of the error terms of the dyad members,
  #       which is exactly what we want because we want to know their effect 
  #       on one another after controlling for their individual tendencies.
  fit0.sociable <- gls(Sociable_AB ~ Adum + Pdum, na.action=na.omit,
              method="REML",verbose=TRUE, correlation=corCompSymm 
              (form=~1|DyadN), data=pw)
  #   
  # round(summary(fit0.sociable)$coefficients,2) # are these the SRM perceiver & target scores?
  #     probably close but not exactly since there are fewer dummy codes that 
  #     represent more participants.
  
  # Extract Rho:
  # Rho = SRM dyadic reciprocity: 
  #       The measure of nonindependence. This is partial intraclass correlation.
  #       That is, it is ‎the correlations between the two members’ DV (sociability)
  #       controlling for the predictors (perceiver, target). 
  #       In the empty model, it's how the dyad members effect each other's DV
  #       after controlling for their individual tendencies = SRM dyadic reciprocity.
  coef(fit0.sociable$model$corStruct,unconstrained=FALSE) # Rho 0.104 = dyadic reciprocity
  summary(fit0.sociable) # DF  324 total; 135 residual
  anova(fit0.sociable)   # DF as expected (Pdum 81; Adum 107). 
                # F-values of Adum = 1.87, Pdum = 1.77 both significant
                # Therefore, Perceiver/target variances will be present (see 
                #   explanation below)
                #   The standardized variances are 0.186, 0.187, respectively 
  
  # Explanation to the dummy codes F values in the empty model from 2018 article:
  # ============================================================================
  # Summary: 
  # The F-values's significance suggest the existence of perceiver/target SRM
  # variance in the data. 
  # When the data is dss, then no such variance is expected. 
  # When the data is ratings, then usually we expect perceiver/target variances.
  # 
  # Kenny says that in the anova command, when the dummy codes F-value is 
  # significant, it means that we expect an actor/partner SRM variance.
  #   (some thinking to figure out that it's SRM variances: Does he mean 
  #     actor/partner of APIM or perceiver/target of SRM? 
  #     If he means actor/partner, then how can he predict it from the empty model
  #     that contains only the persons themselves w/o the predictor?
  #     If he means peceiver/target then it makes more sense (especially because these
  #     are not ds, but raw ratings). So what does it stand for when there are ds? 
  #     In the case of ds no such variance should exist.
  #     OK - the answer is SRM variance!!!! )
  # when we run the anova() test on ds, then the F-value of the aDum and pDum 
  # is zero with pvalue=1.0
  # This is coherent with no Perceiver/target variance for ds, and positive 
  # perceiver/target variances for other types of ratings.

  # partnum|DyadN vs. 1|DyadN ==================================================
  # It's the same.         
      fit1 <- gls(Sociable_AB ~ Adum + Pdum, na.action=na.omit,
                  method="REML",verbose=TRUE, correlation=corCompSymm 
                  (form=~partnum|DyadN), data=pw)
      # round(summary(fit0)$coefficients,2) 
      # summary(fit1)  # Rho 0.104 = the SRM dyadic reciprocity
      # DF  324 total; 135 residual

  # Playing around - looking at the coefficients ===============================
  fit0.attract <- gls(Attractive_AB ~ Adum + Pdum, na.action=na.omit,
              method="REML",verbose=TRUE, correlation=corCompSymm 
              (form=~1|DyadN), data=pw)
  # round(summary(fit0.attract)$coefficients,2)[1:6] # are these the SRM perceiver & target scores?
  #     probably close but not exactly since there are fewer dummy codes that 
  #     represent more participants.
  coef(fit0.attract$model$corStruct,unconstrained=FALSE) # Rho 0.054 = dyadic reciprocity
  summary(fit0.attract) # DF  324 total; 135 residual
  anova(fit0.attract)   # DF as expected (Pdum 81; Adum 107). 
  # F-values of Adum = 6.646, Pdum = 2.582 both very significant
  # Therefore, Perceiver/target variances will be present (see 
  #   explanation above)
  #   The standardized variances are 0.51, 0.185, respectively 
  #   (from a tripleR analysis)
  
  # Take a look at the coefficients of the dummy codes of the empty model:
    # extract the coefficients of the dummy codes just to take a look at it.
    scoefs <- as.data.frame(summary(fit0.sociable)$coefficients)
    scoefs$var <- sub(".*\\$","" ,rownames(scoefs))
    scoefs <- scoefs[-1,] # drop the intercept
    rownames(scoefs) <- NULL
    names(scoefs)[1] <- "val"
    scoefs$actor <- substr(scoefs$var,1,1) == "A"
    sigma(fit0.sociable)^2 # The unexplained variance is 1.13
    vsocial <- var(x$Sociable_AB) # The variable's variance is 1.68
    var(scoefs$val[scoefs$actor])  # The perceivers' variance is 1.32
    var(scoefs$val[!scoefs$actor])  # The targets' variance is 1.57
  
    #same for Attract:
    acoefs <- as.data.frame(summary(fit0.attract)$coefficients)
    acoefs$var <- sub(".*\\$","" ,rownames(acoefs))
    acoefs <- acoefs[-1,] # drop the intercept
    rownames(acoefs) <- NULL
    names(acoefs)[1] <- "val"
    acoefs$actor <- substr(acoefs$var,1,1) == "A"
    sigma(fit0.attract)^2 # The unexplained variance is 1.13
    vattract <- var(x$Attractive_AB) # The variable's variance is 0.46
    var(acoefs$val[acoefs$actor])  # The perceivers' variance is 1.21
    var(acoefs$val[!acoefs$actor])  # The targets' variance is 0.82
    
    # All in all, I have no idea if there's anything I can learn from these 
    # coefficients or the variances.
    

  # Some redundant attempts to avoid dummy codes...=============================
    # If I put the direct actor/partner columns, then ofcourse, I get that it's
    # singular.
    # fit2 <- gls(Sociable_AB ~ ActorN + PartnerN, na.action=na.omit,
    #             method="REML",verbose=TRUE, correlation=corCompSymm 
    #             (form=~partnum|DyadN), data=pw)
    # This is singular. But what if I represent the dummy codes as a number. 
    # Will I be able to get a representation for the actor/partner?
    # I tried turning it into sum(2^the column position). But it created a too big of a 
    # difference between the different actors/partners, so one gets 0 and the other gets
    # like trillion. so, the analysis won't be of any value. 

# The complete model
# ==============================================================================
  
  fit <- gls(Sociable_AB ~ Attractive_AB + Attractive_BA + Adum + Pdum, na.action=na.omit,
          method="REML",verbose=TRUE, correlation=corCompSymm (form=~1|DyadN), data=pw)
  # coef:
  summary(fit)$tTable[2:3,]
  anova(fit)   # DF as expected
               # actor effect (intrapersonal predictor) significant,
               # partner effect (interpersonal predictor) not significant 
               # Adum not significant (p=0.073)
               # Pdum significant (p=0.011)
  # Rho: the correlation between the DVs after controling for the IVs
  #      In the full model, the Rho returns the correlations of the residuals 
  #      of the dyad members' DV after the individual tendencies and the predictors
  coef(fit$model$corStruct,unconstrained=FALSE) # Rho 0.113 = dyadic reciprocity
  summary(fit)$sigma   # sd of residual 1.044
  # The sigma can be used to standardize the APIM actor and partner effects 
  # and hence get them in d:
  summary(fit)$tTable[2:3,1]/summary(fit)$sigma
  
  
# Interaction test
# ==============================================================================
# Kenny points in the article, that the items shouldn't have an interaction with 
# the actor/partner dummy codes.
# Kenny states that if such an interaction exists, then a more complex analysis 
# should be performed. But all the ratings here aren't controlled for the 
# actor/partner. I thought these are the dyadic scores after SRM ran. But it's not
# Most interaction models don't converge. 
# should I perhaps replace the correlation type from symmetry to something else? 
# The pure interaction model does converge and shows a significant F value for 
# the interaction. 
  fit <- gls(Sociable_AB ~ Attractive_AB:Adum, na.action=na.omit,
             method="REML",verbose=TRUE, correlation=corCompSymm (form=~1|DyadN), data=pw)
  #summary(fit)$tTable[2:3,]
  anova(fit)   # we see the interaction has a significant F-Value. 
               # I think it means there is an interaction and the model isn't
               # adjusted to the data. Am I wrong? 
               # Kenny wrote that he got border significance of 0.6 here. 
               # I get totally significant interaction. 
  summary(fit)$sigma   # sd of residual 1.12
  
  fit <- gls(Sociable_AB ~ Attractive_AB:Adum + Attractive_BA:Pdum, na.action=na.omit,
             method="REML",verbose=TRUE, correlation=corCompSymm (form=~1|DyadN), data=pw)
  #summary(fit)$tTable[2:3,]
  anova(fit)   # we see the interaction with Adum has a significant F-Value. 
               # The Pdum interaction isn't significant.
               # I think it means there is an interaction and the model isn't
               # adjusted to the data. Am I wrong? Again, I get totaly significant
               # interaction where Kenny didn't.
               
  # I thought: is it really just a matter of partner-target and actor-perceiver?
  # but when I put all the interactions, it's singular. I tried different
  # combinations, and only this one isn't singular, 
  # Only the _AB:Adum is significant.
  fit <- gls(Sociable_AB ~ Attractive_AB:Adum  +
               Attractive_AB:Pdum, na.action=na.omit,
             method="REML",verbose=TRUE, correlation=corCompSymm (form=~1|DyadN), data=pw)
  anova(fit)   # we see the interaction with Adum has a significant F-Value. 
  

  # fit <- gls(Sociable_AB ~ Attractive_AB*ActorN + Adum + Pdum, na.action=na.omit,
  #            method="REML",verbose=TRUE, correlation=corCompSymm (form=~1|DyadN), data=pw)
  # fit <- gls(Sociable_AB ~ Attractive_AB*Adum + Adum + Pdum, na.action=na.omit,
  #            method="REML",verbose=TRUE, correlation=corCompSymm (form=~1|DyadN), data=pw)
  # fit <- gls(Sociable_AB ~ Attractive_AB*Adum, na.action=na.omit,
  #            method="REML",verbose=TRUE, correlation=corCompSymm (form=~1|DyadN), data=pw)
  
  
# The degrees of freedom DF, F test, dyadic reciprocity
# ==============================================================================
  # The equations for the above are given in the article for constant group size
  # and no missing values. 
  # Here I try to adjust the equations for more flexible data as is the case in 
  # most of our data.
  
  # original equations for uniform group size:
  # ==========================================
    ng = 27 # number of groups
    np = 4  # group size
    dfn = ((np-1)*(np-2)/2 -1)* ng
    dfd = (np -1)*(np-2)*ng/2
    # F: calculate the rho
    fit0 <- gls(Sociable_AB ~ Adum + Pdum, na.action=na.omit,
                method="REML",verbose=TRUE, correlation=corCompSymm
                (form=~1|DyadN), data=pw)
    rho <- as.numeric(coef(fit0$model$corStruct,unconstrained=FALSE))
    (F <- (1+rho)/(1-rho))
    plow=pf(F,dfn,dfd)
    2 * min(plow, 1 - plow)
    
  # F, rho, dyadic reciprocity equations (un-even groups with missing values)
  # =========================================================================
  # In the case of missing values, 
  #   the calculation of degrees of freedom might be a bit biased 
  #   according to the selection of dyads.
  #   Assuming the missing values are of random pattern, the calculation is acceptable.
  # 
  # The calculation of degrees of freedom:
  # for each group: remove one actor and two partners, calculate no. of dyads
  # f.dfd = The degrees of freedom of dyads 
  #         after accounting for perceivers and targets:
  #         the number of dyads after removing a participant and a partner from 
  #         each group.
  # f.dfn = The degrees of freedom after accounting for the number of groups = 
  #       =  f.dfd - number of groups
  
  f.dfd <- pw %>%
      group_by(Group) %>%
      # Randomly select one ActorN to filter out associated rows as actor/partner
      mutate(ap_to_remove = sample(unique(ActorN), 1)) %>%
      filter(!ActorN %in% ap_to_remove, !PartnerN %in% ap_to_remove) %>%
      # Randomly select an additional PartnerN to filter out associated rows
      filter(!PartnerN %in% sample(unique(PartnerN), 1)) %>%
      ungroup() %>%
      # Count unique Dyads
      summarise(distinct_dyads = n_distinct(DyadN)) %>%
      pull(distinct_dyads)
    
  ng <- length(unique(pw$Group)) #number of groups
  f.dfn <- f.dfd-ng
  
  # calculate the quantile using the rho of the model:
  # The rho is the nonindepencdence between the DV of the dyad members after
  # controlling for the individual tendencies (dummy codes) and the predictors
  fit <- gls(Sociable_AB ~ Attractive_AB + Attractive_BA + Adum + Pdum, 
             na.action=na.omit,
             method="REML",verbose=TRUE, correlation=corCompSymm
             (form=~1|DyadN), data=pw)
  rho <- as.numeric(coef(fit$model$corStruct,unconstrained=FALSE))
  (F <- (1+rho)/(1-rho))
  
  # The probability to get the F given our degrees of freedom:
  plow <- pf(F,f.dfn,f.dfd)
  
  # The significance of the model:
  2 * min(plow, 1 - plow)
  
  cat("The rho of this APIM is: ", round(rho, 2), 
      "\nIt represents the dependency between the dyad member's DV",
      "\n   after controlling for the APIM predictors and the individual tendencies.",
      "\nThe significance of the rho is: p =", round(2 * min(plow, 1 - plow),3) )
  
  # A failing attemp to imitate the DFs created in SPSS
  # ==============================================================================
  # In the article, the SPSS creates dummy codes automatically (apparently) from
  # the categorical variables. 
  # The degrees of freedom it creates are 81 for both the actor and the partner. 
  # Our code generates dummy codes with 81 dfs for the partner, but 107 for the
  # actors.
  # So, I tried removing more dummy codes from the actor as well. 
  # But I think that somehow the SPSS operates it differently, because the 
  # analysis is different. Anyway, I tried it, so I keep it...
  
  lpw <- pw
  # Create less dummy codes for the actor as well
  lPdum <- Pdum
  lAdum <- model.matrix(~pw$ActorN)
  lAdum <- lAdum[, duplicated(pw[!duplicated(pw$ActorN),"Group"])]
  
  # Add the dummy codes into the dataset:
  lpw$Adum <- lAdum
  lpw$Pdum <- lPdum
  lfit0 <- gls(Sociable_AB ~ Adum + Pdum, na.action=na.omit,
               method="REML",verbose=TRUE, correlation=corCompSymm 
               (form=~1|DyadN), data=lpw)
  
  # We see that the intercept, the Rho, and the coefs are all different. Different
  # from both the analysis with the regular dummies, and from Kenny's SPSS analysis:
  
  # round(summary(lfit0)$coefficients,2)[1:6] 
  coef(lfit0$model$corStruct,unconstrained=FALSE) # Rho 0.251 = dyadic reciprocity
  summary(lfit0) # DF  324 total; 161 residual
  anova(lfit0)   # DF are both 81, with different F values.
  
  